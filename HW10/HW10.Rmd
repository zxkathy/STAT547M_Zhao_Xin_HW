---
title: "HW10 Data from the Web"
output: github_document
---
## Pre-Work

Load the required packages needed for the assignment.

```{r message=FALSE, warning=FALSE}
library(httr)
library(dplyr)
library(glue)
library(purrr)
library(jsonlite)
library(tidyr)
```


## Task 1: Make API queries "by hand" using `httr`

API info is obtained from the website [`Star Wars API`](https://swapi.co). I noticed for this particular website, the authorization token is nor required when requiring a API.

But in fact, there are some websites requesting the authorization token. Here is one which we have illustrated in the class.

```{r}
thor_result <- httr::GET(url = "http://www.omdbapi.com/?t=Thor&y=2017&apikey=df5f9e85")
thor_content <- content(thor_result)

thor_content %>%
	head() 
```

#### Simple example

There are several infomation we can search in the swqpi.co website, we can input a particular Star Wars `character` by the number, and a sequence of info regarding that `character` will appear, such as name, height, gender, links of the `films` this `character` is in, links of the `starships`. Alternatively, we can input a `film` number, as before, detailed info of that film will appear, such as, film name, film release date, link os `characters` who are involved in that film. Similarly, the `starships` inquery include info such as starship name, who were in the ship, and this ship appears in which movie. 

As you can see, each webpage is not on its own. There exist some relationships in between.

First, we will try our function in the `vehicles` search and see whether we can get some result from the following simple code.

```{r, results='asis'}
veh_res <- httr::GET(url = "https://swapi.co/api/vehicles/4/")
veh_content <- content(veh_res)
veh_content %>%
	head()
```


#### Function to retrieve detailed info

```{r}
getChar_info <- function(number){
	res <- glue("https://swapi.co/api/people/{number}/") %>% 
		GET() %>% 
		content()
	return(res)
}
```


In the next chunk, I will demostrate how to use the info to generate a nice and tidy dataframe. 

The idea of my dataframe is to list out the character, and the movies that character was in, also, within each movies, I need to find out the `starship(s)` that are relevant to the `character`. Note: I am not going to list all the `starship(s)` appear in the movie that `character` was in, which means even though the `starship(s)` and `character` are in the same film of a particular year, there might be a chance that the `character` has no relationship with the `starship(s)`.


I intend to get the infomation of `name`, `gender`, and `species` directly from the webpage of `character/people`, and get the information of `film_name` and `release_year` from the webpage `films`. As for the `starship(s)` which are relevant to the `character`, I will first find all the `starship(s)` that are relevant to the `character` aross all the Star Wars films, and then categorize them into different `films`, and finally match with the existing categorized `character`. 


```{r, results='asis'}
moreInfo <- function(oneList, title = "title"){
	if(title == "title"){
		unlist(oneList$title)
	}else if(title == "year"){
		unlist(oneList$release_date)
	}else if(title == "films"){
		unlist(oneList$films)
	}else if(title == "shipname"){
		unlist(oneList$name)
	}
}


convertTibble <- function(list){
	listSpecies <- fromJSON(unlist(list$species))
	filmInfo <- map(list$films, fromJSON)
	listFilms <- map(filmInfo, "title", moreInfo)
	listYears <- map2(filmInfo, "year", moreInfo) %>%
		substr(1,4)
	
	
	#listStarships
	starshipInfo <- map(list$starships, fromJSON)
	shipFilmWeb <- map(starshipInfo, "films", moreInfo)
	
	film_starship_df <-
		tibble(shipname = map2(starshipInfo, "shipname", moreInfo) %>%
					 	unlist(),
					 filmWeb = shipFilmWeb) %>%
		unnest()
	
	#listFilms
	cha_film_df <-
		tibble(
			name = list$name,
			gender = list$gender,
			species = listSpecies$name,
			filmWeb = unlist(list$films),
			film_name = unlist(listFilms),
			release_year = unlist(listYears)
		)
	
	left_join(cha_film_df,
						aggregate(film_starship_df[1], film_starship_df[2], unique),
						by = "filmWeb") %>%
		mutate(shipNum = map_int(shipname, length)) %>%
		select(name:species, film_name, release_year, shipNum, shipname) %>%
		arrange(release_year) %>%
		return()
}

char1 <- convertTibble(getChar_info("1"))

char1 %>%
	knitr::kable()
```

It could be seen that the character of `Luke Skywalker` appears in 5 films of the Star Wars series, and the year range was from 1997 to 2015, also, in the first few editions of the films, the character had/travelled in some starships, but in latter years, he has no relationship with any starships. 


## Task 2: Scrape data

#### Pre-Work

```{r, message=FALSE, warning=FALSE}
library(rvest)
vignette("selectorgadget")
```

#### Taks2.1  Review Slides

Work through the [final set of slides](https://github.com/ropensci/user2016-tutorial/blob/master/03-scraping-data-without-an-api.pdf) from the rOpenSci UseR! 2016 workshop. This will give you basic orientation, skills, and pointers on the rvest package.

```{r}
frozen <- read_html("http://www.imdb.com/title/tt2294629/")
itals <- html_nodes(frozen, "em")
html_text(itals)
html_name(itals)
html_children(itals)
html_attr(itals, "class")
html_attrs(itals)
```

```{r}
cast2 <- html_nodes(frozen, "#titleCast span.itemprop")
html_text(cast2)
```

```{r}
cast3 <- html_nodes(frozen, ".itemprop .itemprop")
html_text(cast3)
```


```{r}
kw <- read_html("https://www.bestplaces.net/cost_of_living/city/florida/key_west")
col <- html_nodes(kw, css = "#mainContent_dgCostOfLiving tr:nth-child(2) td:nth-child(2)")
html_text(col)
```

```{r}
tables <- html_nodes(kw, css = "table")
html_table(tables, header = TRUE, fill = TRUE)[[2]]
```

```{r}
kw2 <- read_html("https://www.bestplaces.net/climate/city/florida/key_west")
climate <- html_nodes(kw2, css = "table")
html_table(climate, header = TRUE, fill = TRUE)[[2]]
```


#### Taks2.2  Scrap Your Own Data

In this task, I will analyze the recent listed house information located in the central location of Vancouver (i.e. Downtown Vancouver West) and central location of Burnaby (i.e. Metrotown) obtained from  [`The Rick Clarke Team`](http://www.realestatevalley.ca/). I will detailed break down my solution when analyzing the info for Downtown Vancouver West, and will bulk extract info for Metrotown.

```{r, results='asis'}
source("read_dat.R")
info_van <- info_bur <- list()
df_van <- df_bur <- c()
for(i in 1:length(h)){
	info_van[[i]] <- html_nodes(h[[i]], css = "dd")
	info_text <- html_text(info_van[[i]])
	pageHouse <- matrix(NA, nrow = length(info_text)/9, ncol = 9) %>%
			data.frame()
	for(j in 1:length(info_text)){
		pageHouse[floor(j/10 + 1), ifelse(j %% 9 == 0, 9, j %% 9)] <- info_text[j]
	}
	df_van <- bind_rows(df_van, pageHouse %>%na.omit())
}

df_van[1,] %>%
	knitr::kable()
```

I noticed even though we have got infomation from the website, the colnames info is still missing. So, we need to extract info from the website again for the colnames.

```{r}
cap <- read_html("http://www.realestatevalley.ca/burnaby-real-estate.php#detailed")
name <- 
	html_nodes(cap, "#listing-R2221141 dt") %>%
	html_text()
name
```

After extract the plain text info of column names, I found it is not in good format, i.e. there are `:`s after the names, so I am extract the character info before `:`s. 

```{r}
names(df_van) <- name %>%
	substr(1,regexpr(":", name)-1)

dim(df_van)
```

After renaming the data frame, we got 92 rows, which means we got 92 houses' info. Let's have a look at the head part of data frame. 

Before using the data frame `df_van` doing any analysis, we need to change to character columns to number columns, such as `Price`, `Beds`, `Baths`

```{r, results='asis'}
remDol <- function(char){
	gsub(",","",char) %>%
		substr(2, nchar(char))
}

van_house_dat <- 
	df_van %>%
	mutate(BedN = as.numeric(df_van$Beds),
				 BathN = as.numeric(df_van$Baths),
				 PriceChar = unlist(map(df_van$Price, remDol)), 
				 PriceInK = round(as.numeric(PriceChar)/1000, 0)) %>%
	select(`MLS® #`, PriceInK, BedN:BathN, Type, Address, Subdivision, Area)

van_house_dat %>%
	head(10) %>%
	knitr::kable()
```

Let's analyze the Burnaby's data.

```{r}
for(i in 1:length(b)){
	info_bur[[i]] <- html_nodes(b[[i]], css = "dd")
	info_text <- html_text(info_bur[[i]])
	pageHouse <- matrix(NA, nrow = length(info_text)/9, ncol = 9) %>%
		data.frame()
	for(j in 1:length(info_text)){
		pageHouse[floor(j/10 + 1), ifelse(j %% 9 == 0, 9, j %% 9)] <- info_text[j]
	}
	df_bur <- bind_rows(df_bur, pageHouse %>%na.omit())
}

names(df_bur) <- name %>%
	substr(1,regexpr(":", name)-1)

bur_house_dat <-
	df_bur %>%
	mutate(BedN = as.numeric(df_bur$Beds),
				 BathN = as.numeric(df_bur$Baths),
				 PriceChar = unlist(map(df_bur$Price, remDol)),
				 PriceInK = round(as.numeric(PriceChar)/1000, 0)) %>%
	select(`MLS® #`, PriceInK, BedN:BathN, Type, Address, Subdivision, Area)

full_house_dat <- rbind(van_house_dat, bur_house_dat)
```


Next, let's take use of this cleaned data frame and do some statistical analysis.

(1)

```{r}

```

